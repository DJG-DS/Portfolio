{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import datetime\n",
        "import json\n",
        "import yaml\n",
        "import azure.ai.ml\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input, Output\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml.entities import (\n",
        "    BatchEndpoint,\n",
        "    BatchDeployment,\n",
        "    AmlCompute,\n",
        "    PipelineComponentBatchDeployment,\n",
        "    Environment,\n",
        ")\n",
        "\n",
        "print(f\"SDK version: {azure.ai.ml.__version__}\")\n",
        "\n",
        "# Set your subscription, resource group and workspace name:\n",
        "subscription_id = \"25f559b2-3a60-46ab-bf85-6e8a8359d5e4\"\n",
        "resource_group = \"Peak-rg-MIWG\"\n",
        "workspace = \"forecasting_demo\"\n",
        "\n",
        "# connect to the AzureML workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
        ")\n",
        "\n",
        "print(ml_client)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.8.0\nMLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7fc2e6e56aa0>,\n         subscription_id=25f559b2-3a60-46ab-bf85-6e8a8359d5e4,\n         resource_group_name=Peak-rg-MIWG,\n         workspace_name=forecasting_demo)\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093406284
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Compute"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify aml compute name.\n",
        "cpu_compute_target = \"forecasting-compute-cluster\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_DS11_V2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=1,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named forecasting-compute-cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093406528
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Set the input and output URI paths for the data. Supported paths include:\n",
        "# local: `./<path>\n",
        "# Blob: wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>\n",
        "# ADLS: abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>\n",
        "# Datastore: azureml://datastores/<data_store_name>/paths/<path>\n",
        "# Data Asset: azureml:<my_data>:<version>\n",
        "# As an example, we set the input path to a file on a public blob container\n",
        "# As an example, we set the output path to a folder in the default datastore\n",
        "# ==============================================================\n",
        "input_path = \"azureml:MasterSourceData:1\"\n",
        "output_path = \"azureml://datastores/forecastingdemosourcedatalake/paths/outputfiles/\"\n",
        "\n",
        "# ==============================================================\n",
        "# What type of data are you pointing to?\n",
        "# AssetTypes.URI_FILE (a specific file)\n",
        "# AssetTypes.URI_FOLDER (a folder)\n",
        "# AssetTypes.MLTABLE (a table)\n",
        "# The path we set above is a specific file\n",
        "# ==============================================================\n",
        "data_type = AssetTypes.URI_FILE\n",
        "output_type = AssetTypes.URI_FOLDER\n",
        "\n",
        "# ==============================================================\n",
        "# Set the input mode. The most commonly-used modes:\n",
        "# InputOutputModes.RO_MOUNT\n",
        "# InputOutputModes.DOWNLOAD\n",
        "# Set the mode to Read Only (RO) to mount the data\n",
        "# ==============================================================\n",
        "input_mode = InputOutputModes.RO_MOUNT\n",
        "\n",
        "# ==============================================================\n",
        "# Set the output mode. The most commonly-used modes:\n",
        "# InputOutputModes.RW_MOUNT\n",
        "# InputOutputModes.UPLOAD\n",
        "# Set the mode to Read Write (RW) to mount the data\n",
        "# ==============================================================\n",
        "output_mode = InputOutputModes.RW_MOUNT"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093406629
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Job Environment for Pipeline Steps"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dependencies_dir = \"./src/dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093406708
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {dependencies_dir}/conda.yaml\n",
        "name: forecast\n",
        "channels:\n",
        "- conda-forge\n",
        "- anaconda\n",
        "dependencies:\n",
        "- python=3.8\n",
        "- pip=22.1.2\n",
        "- numpy~=1.22.3\n",
        "- pandas~=1.3.5\n",
        "- scikit-learn=1.2.2\n",
        "- py-xgboost=1.3.3\n",
        "- holidays=0.10.3\n",
        "- cudatoolkit=11.1\n",
        "- pyopenssl=23.2.0\n",
        "- psutil>=5.2.2,<6.0.0\n",
        "- GitPython=3.1.32\n",
        "- tqdm\n",
        "- setuptools=65.5.1\n",
        "- wheel=0.38.1\n",
        "- openssl=1.1.1s\n",
        "- gunicorn\n",
        "- flask\n",
        "- pip:\n",
        "  - inference-schema\n",
        "  - azure-ai-ml\n",
        "  - fsspec\n",
        "  - azureml-mlflow==1.53.0\n",
        "  - azureml-defaults==1.53.0\n",
        "  - azureml-telemetry==1.53.0\n",
        "  - azureml-interpret==1.53.0\n",
        "  - cryptography>=41.0.2\n",
        "  - scipy==1.10.1\n",
        "  - spacy==2.1.8\n",
        "  - prophet==1.1.4\n",
        "  - py-cpuinfo==5.0.0 "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/dependencies/conda.yaml\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_env_name = \"forecastingDemo_pipline_env\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for forecasting demo pipeline\",\n",
        "    tags={},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"1\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name forecastingDemo_pipline_env is registered to workspace, the environment version is 1\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093407072
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Component 1: Training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_src_dir = \"./src/components/train\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093407181
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.py\n",
        "\n",
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "from prophet import Prophet\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Define the output directory\n",
        "output_dir = './outputs'\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "# input and output arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--sensor_data\", type=str, help=\"path to sensor data\")\n",
        "parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "sensor_data = pd.read_csv(args.sensor_data)\n",
        "\n",
        "sensors = sensor_data[\"Sensor Name\"].unique()\n",
        "\n",
        "for sensor in sensors:\n",
        "    try:\n",
        "\n",
        "        '''\n",
        "        clean dataset by replacing suspect readings with the mean average of the dataset\n",
        "\n",
        "        '''\n",
        "        # get the DataFrame for the current sensor\n",
        "        df_sensor = sensor_data[sensor_data[\"Sensor Name\"] == sensor]\n",
        "\n",
        "        # calculate the mean of non-zero, non-negative, and non-empty values\n",
        "        mean_value = df_sensor[df_sensor[\"Value\"] > 0][\"Value\"].mean()\n",
        "\n",
        "        # replace the values for rows that are zero, negative, or na\n",
        "        df_sensor.loc[(df_sensor[\"Value\"] <= 0) | (df_sensor[\"Value\"].isna()), \"Value\"] = mean_value\n",
        "\n",
        "        # replace the empty rows with the mean value\n",
        "        df_sensor[\"Value\"].fillna(mean_value, inplace=True)\n",
        "\n",
        "        # update the original DataFrame with the modified values\n",
        "        sensor_data[sensor_data[\"Sensor Name\"] == sensor] = df_sensor\n",
        "\n",
        "\n",
        "        '''\n",
        "        fit data to model\n",
        "\n",
        "        '''\n",
        "\n",
        "        # get the DataFrame for the current sensor\n",
        "        x = pd.to_datetime(df_sensor[\"Timestamp\"])\n",
        "        y = df_sensor.Value\n",
        "\n",
        "        # Create a dictionary with the data for each column\n",
        "        data = {'ds': x, 'y': y}\n",
        "\n",
        "        # Create the DataFrame\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Create and fit the Prophet model with custom seasonality\n",
        "        model = Prophet(\n",
        "            daily_seasonality=True,\n",
        "            interval_width=0.8,\n",
        "            changepoint_prior_scale=0.15,\n",
        "            seasonality_prior_scale=0.1,\n",
        "            uncertainty_samples=1000\n",
        "        )\n",
        "\n",
        "        # modify seasonality and fourier_order (daily)\n",
        "        model.add_seasonality(\n",
        "            name='daily',\n",
        "            period=1,\n",
        "            fourier_order=50 \n",
        "        )\n",
        "\n",
        "        # modify seasonality and fourier_order (weekly)\n",
        "        model.add_seasonality(\n",
        "            name='weekly',\n",
        "            period=7,\n",
        "            fourier_order=1\n",
        "        )\n",
        "\n",
        "        # modify seasonality and fourier_order\n",
        "        model.add_seasonality(\n",
        "            name='fortnight',    period=15,\n",
        "            fourier_order=10 \n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        forecast_model = model.fit(df)\n",
        "\n",
        "        '''\n",
        "        save model as a pickle file\n",
        "\n",
        "        '''\n",
        "        # Specify the file path where you want to save the pickle file\n",
        "        file_path = sensor + \"_forecast_model.pkl\"\n",
        "        pickle_file_path = os.path.join(output_dir, file_path)\n",
        "\n",
        "        # Save the model as a pickle file\n",
        "        with open(pickle_file_path, 'wb') as file:\n",
        "            pickle.dump(forecast_model, file)\n",
        "\n",
        "        # Close the file\n",
        "        file.close()\n",
        "\n",
        "        # Specify the file path where you want to save the pickle file\n",
        "        file_path = sensor + \"_forecast_model.pkl\"\n",
        "        pickle_file_path = os.path.join(args.model, file_path)\n",
        "\n",
        "        # Save the model as a pickle file\n",
        "        with open(pickle_file_path, 'wb') as file:\n",
        "            pickle.dump(forecast_model, file)\n",
        "\n",
        "        # Close the file\n",
        "        file.close()\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(sensor + \" has error:\", str(e))\n",
        "        continue          "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/components/train/train.py\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.yml\n",
        "# <component>\n",
        "name: forecastdemo_model\n",
        "display_name: ForecastDemo Model\n",
        "type: command\n",
        "inputs:\n",
        "  sensor_data: \n",
        "    type: uri_folder  \n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: uri_folder\n",
        "code: ./\n",
        "environment: azureml:forecastingDemo_pipline_env:1\n",
        "command: >-\n",
        "  python train.py \n",
        "  --sensor_data ${{inputs.sensor_data}}  \n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/components/train/train.yml\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the component from the yml file\n",
        "train_component = load_component(source=os.path.join(train_src_dir, \"train.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component forecastdemo_model with Version 2023-11-27-13-56-47-8929439 is registered\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093409031
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Component 2: Score"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_src_dir = \"./src/components/score\"\n",
        "os.makedirs(score_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093409258
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {score_src_dir}/score.py\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pathlib import Path\n",
        "import mlflow\n",
        "\n",
        "# start logging\n",
        "mlflow.start_run()\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model\", type=str, help=\"Path to the input model directory\")\n",
        "parser.add_argument(\"--data\", type=str, help=\"Path to the data to score\")\n",
        "parser.add_argument(\"--scoredata\", type=str, help=\"Path to save the score data\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "lines = [\n",
        "    f\"model path: {args.model}\",\n",
        "    f\"input data path: {args.data}\",\n",
        "    f\"Output data path: {args.scoredata}\",\n",
        "]\n",
        "\n",
        "for line in lines:\n",
        "    print(f\"\\t{line}\")\n",
        "\n",
        "sensor_data = pd.read_csv(args.data)\n",
        "sensors = sensor_data[\"Sensor Name\"].unique()\n",
        "\n",
        "# Define the output directory for the first directory\n",
        "# output_dir = './outputs'\n",
        "# output_dir = \"abfss://forecastingdemooutputs@stdtlkforecastingdemo.dfs.core.windows.net/outputfiles\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "sensor_locs = sensor_data[[\"Sensor Name\", \"Sensor Centroid Latitude\", \"Sensor Centroid Longitude\"]]\n",
        "sensor_locs = sensor_locs.drop_duplicates()\n",
        "# Save the predictions to the specified path\n",
        "sensor_locs_file_path = os.path.join(args.scoredata, 'sensor_locs.csv')\n",
        "sensor_locs.to_csv(sensor_locs_file_path, index=False)\n",
        "\n",
        "for sensor in sensors:\n",
        "    try:\n",
        "        # Construct the path to the pickle file for the current sensor\n",
        "        model_file_name = f\"{sensor}_forecast_model.pkl\"\n",
        "        model_file_path = os.path.join(args.model, model_file_name)\n",
        "\n",
        "        # Load the trained model for the current sensor\n",
        "        with open(model_file_path, 'rb') as model_file:\n",
        "            model = pickle.load(model_file)\n",
        "\n",
        "        # Generate predictions\n",
        "        future = model.make_future_dataframe(periods=14, freq='D')\n",
        "        forecast = model.predict(future)\n",
        "        fortnight_forecast = forecast.tail(14)\n",
        "        fortnight_forecast['sensor reading'] = fortnight_forecast['yhat']\n",
        "        fortnight_forecast[\"Sensor Name\"] = sensor\n",
        "    \n",
        "\n",
        "        # Save the predictions to the specified path\n",
        "        output_file_path = os.path.join(args.scoredata, f'{sensor}_predictions.csv')\n",
        "        fortnight_forecast.to_csv(output_file_path, index=False)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{sensor} has error:\", str(e))\n",
        "        pass\n",
        "\n",
        "mlflow.end_run()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/components/score/score.py\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699390862328
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {score_src_dir}/score.yml\n",
        "name: forecastdemo_model_score\n",
        "display_name: ForecastDemo Model Scoring\n",
        "type: command\n",
        "inputs:\n",
        "  model:\n",
        "    type: uri_file\n",
        "  data:\n",
        "    type: uri_file\n",
        "outputs:\n",
        "  scoredata:\n",
        "    type: uri_folder\n",
        "code: ./\n",
        "environment: azureml:forecastingDemo_pipline_env:1\n",
        "command: >-\n",
        "  python score.py\n",
        "  --model ${{inputs.model}}\n",
        "  --data ${{inputs.data}}\n",
        "  --scoredata ${{outputs.scoredata}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/components/score/score.yml\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698928126722
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "score_component = load_component(source=os.path.join(score_src_dir, \"score.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "score_component = ml_client.create_or_update(score_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {score_component.name} with Version {score_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component forecastdemo_model_score with Version 2023-11-27-13-56-49-3496529 is registered\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093410621
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Pipeline from Components"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@pipeline\n",
        "def forecastingdemo_pipeline(\n",
        "    pipeline_job_data_input: Input,\n",
        "    pipeline_job_registered_model_name: str,\n",
        "):\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_component(\n",
        "        sensor_data=pipeline_job_data_input,\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # Adding scoring component\n",
        "    score_job = score_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        model=train_job.outputs.model,\n",
        "    )\n",
        "\n",
        "    # A pipeline returns a dictionary of outputs\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": train_job.outputs.model,\n",
        "        \"pipeline_job_score_predictions\": score_job.outputs.scoredata,\n",
        "    }\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093411084
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"forecastingdemo_model\"\n",
        "\n",
        "# Let's instantiate the pipeline\n",
        "pipeline_job = forecastingdemo_pipeline(\n",
        "    pipeline_job_data_input=Input(type=data_type, path=input_path, mode=input_mode),\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")\n",
        "\n",
        "pipeline_job.outputs.pipeline_job_score_predictions = Output(\n",
        "        type=output_type, mode=output_mode, path=output_path\n",
        "    )\n",
        "\n",
        "# set pipeline level compute\n",
        "pipeline_job.settings.default_compute = cpu_compute_target\n",
        "pipeline_job.settings.force_rerun = 'true'\n",
        "pipeline_job.settings.continue_on_step_failure = 'false'\n",
        "pipeline_job.display_name = 'DemoForecasting_EndtoEnd'\n",
        "pipeline_job.description = 'Forecasting Demo End to End'"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093411353
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipeline_job)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "display_name: DemoForecasting_EndtoEnd\ndescription: Forecasting Demo End to End\ntype: pipeline\ninputs:\n  pipeline_job_data_input:\n    mode: ro_mount\n    type: uri_file\n    path: azureml:MasterSourceData:1\n  pipeline_job_registered_model_name: forecastingdemo_model\noutputs:\n  pipeline_job_train_data:\n    type: uri_folder\n  pipeline_job_score_predictions:\n    mode: rw_mount\n    type: uri_folder\n    path: azureml://datastores/forecastingdemosourcedatalake/paths/outputfiles/\njobs:\n  train_job:\n    type: command\n    inputs:\n      sensor_data:\n        path: ${{parent.inputs.pipeline_job_data_input}}\n      registered_model_name:\n        path: ${{parent.inputs.pipeline_job_registered_model_name}}\n    outputs:\n      model: ${{parent.outputs.pipeline_job_train_data}}\n    resources:\n      instance_count: 1\n    component:\n      name: forecastdemo_model\n      version: 2023-11-27-13-56-47-8929439\n      display_name: ForecastDemo Model\n      type: command\n      inputs:\n        sensor_data:\n          type: uri_folder\n          optional: false\n        registered_model_name:\n          type: string\n          optional: false\n      outputs:\n        model:\n          type: uri_folder\n      command: python train.py  --sensor_data ${{inputs.sensor_data}}   --registered_model_name\n        ${{inputs.registered_model_name}}  --model ${{outputs.model}}\n      environment: azureml:/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/environments/forecastingDemo_pipline_env/versions/1\n      code: azureml:/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/codes/9e2b46bf-b0a9-4a11-8f46-0ad71b369f1e/versions/1\n      resources:\n        instance_count: 1\n      id: /subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/components/forecastdemo_model/versions/2023-11-27-13-56-47-8929439\n      creation_context:\n        created_at: '2023-11-27T13:56:48.539069+00:00'\n        created_by: Carla Marques Fiadeiro\n        created_by_type: User\n        last_modified_at: '2023-11-27T13:56:48.613125+00:00'\n        last_modified_by: Carla Marques Fiadeiro\n        last_modified_by_type: User\n      is_deterministic: true\n  score_job:\n    type: command\n    inputs:\n      model:\n        path: ${{parent.jobs.train_job.outputs.model}}\n      data:\n        path: ${{parent.inputs.pipeline_job_data_input}}\n    outputs:\n      scoredata: ${{parent.outputs.pipeline_job_score_predictions}}\n    resources:\n      instance_count: 1\n    component:\n      name: forecastdemo_model_score\n      version: 2023-11-27-13-56-49-3496529\n      display_name: ForecastDemo Model Scoring\n      type: command\n      inputs:\n        model:\n          type: uri_file\n          optional: false\n        data:\n          type: uri_file\n          optional: false\n      outputs:\n        scoredata:\n          type: uri_folder\n      command: python score.py --model ${{inputs.model}} --data ${{inputs.data}} --scoredata\n        ${{outputs.scoredata}}\n      environment: azureml:/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/environments/forecastingDemo_pipline_env/versions/1\n      code: azureml:/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/codes/dc5d06bc-7ae4-4d87-9b14-45fa811d28c2/versions/1\n      resources:\n        instance_count: 1\n      id: /subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/components/forecastdemo_model_score/versions/2023-11-27-13-56-49-3496529\n      creation_context:\n        created_at: '2023-11-27T13:56:50.242950+00:00'\n        created_by: Carla Marques Fiadeiro\n        created_by_type: User\n        last_modified_at: '2023-11-27T13:56:50.311637+00:00'\n        last_modified_by: Carla Marques Fiadeiro\n        last_modified_by_type: User\n      is_deterministic: true\nsettings:\n  default_compute: azureml:forecasting-compute-cluster\n  continue_on_step_failure: false\n  force_rerun: true\n\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093411502
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit Job"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job,\n",
        "    # Project's name\n",
        "    experiment_name=\"forecastdemo_experiment\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: willing_yacht_2hc0wwd7dn\nWeb View: https://ml.azure.com/runs/willing_yacht_2hc0wwd7dn?wsid=/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourcegroups/Peak-rg-MIWG/workspaces/forecasting_demo\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-11-27 13:56:55Z] Submitting 1 runs, first five are: 3c3de4db:6144b836-bcba-4ba7-aa50-6a0ab72e04b0\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701093374620
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}