{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import datetime\n",
        "import json\n",
        "import yaml\n",
        "import azure.ai.ml\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input, Output\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml.entities import (\n",
        "    BatchEndpoint,\n",
        "    BatchDeployment,\n",
        "    AmlCompute,\n",
        "    PipelineComponentBatchDeployment,\n",
        "    Environment,\n",
        ")\n",
        "\n",
        "print(f\"SDK version: {azure.ai.ml.__version__}\")\n",
        "\n",
        "# Set your subscription, resource group and workspace name:\n",
        "subscription_id = \"25f559b2-3a60-46ab-bf85-6e8a8359d5e4\"\n",
        "resource_group = \"Peak-rg-MIWG\"\n",
        "workspace = \"forecasting_demo\"\n",
        "\n",
        "# connect to the AzureML workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
        ")\n",
        "\n",
        "print(ml_client)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.8.0\nMLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f1b541bbc40>,\n         subscription_id=25f559b2-3a60-46ab-bf85-6e8a8359d5e4,\n         resource_group_name=Peak-rg-MIWG,\n         workspace_name=forecasting_demo)\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699439236127
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Compute"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify aml compute name.\n",
        "cpu_compute_target = \"forecasting-compute-cluster\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_DS11_V2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=1,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named forecasting-compute-cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699439236445
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Set the input and output URI paths for the data. Supported paths include:\n",
        "# local: `./<path>\n",
        "# Blob: wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>\n",
        "# ADLS: abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>\n",
        "# Datastore: azureml://datastores/<data_store_name>/paths/<path>\n",
        "# Data Asset: azureml:<my_data>:<version>\n",
        "# As an example, we set the input path to a file on a public blob container\n",
        "# As an example, we set the output path to a folder in the default datastore\n",
        "# ==============================================================\n",
        "input_path = \"azureml:MasterSourceData:1\"\n",
        "output_path = \"azureml://datastores/forecastingdemosourcedatalake/paths/outputfiles/data.csv\"\n",
        "# output_path = \"abfss://forecastingdemooutputs@stdtlkforecastingdemo.dfs.core.windows.net/outputfiles/output.csv\"\n",
        "\n",
        "# ==============================================================\n",
        "# What type of data are you pointing to?\n",
        "# AssetTypes.URI_FILE (a specific file)\n",
        "# AssetTypes.URI_FOLDER (a folder)\n",
        "# AssetTypes.MLTABLE (a table)\n",
        "# The path we set above is a specific file\n",
        "# ==============================================================\n",
        "data_type = AssetTypes.URI_FILE\n",
        "\n",
        "# ==============================================================\n",
        "# Set the input mode. The most commonly-used modes:\n",
        "# InputOutputModes.RO_MOUNT\n",
        "# InputOutputModes.DOWNLOAD\n",
        "# Set the mode to Read Only (RO) to mount the data\n",
        "# ==============================================================\n",
        "input_mode = InputOutputModes.RO_MOUNT\n",
        "\n",
        "# ==============================================================\n",
        "# Set the output mode. The most commonly-used modes:\n",
        "# InputOutputModes.RW_MOUNT\n",
        "# InputOutputModes.UPLOAD\n",
        "# Set the mode to Read Write (RW) to mount the data\n",
        "# ==============================================================\n",
        "output_mode = InputOutputModes.RW_MOUNT"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699439236705
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Job Environment for Pipeline Steps"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dependencies_dir = \"./src/dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699439237002
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {dependencies_dir}/conda.yaml\n",
        "name: forecast\n",
        "channels:\n",
        "- conda-forge\n",
        "- anaconda\n",
        "dependencies:\n",
        "- python=3.8\n",
        "- pip=22.1.2\n",
        "- numpy~=1.22.3\n",
        "- pandas~=1.3.5\n",
        "- py-xgboost=1.3.3\n",
        "- holidays=0.10.3\n",
        "- cudatoolkit=11.1\n",
        "- pyopenssl=23.2.0\n",
        "- psutil>=5.2.2,<6.0.0\n",
        "- GitPython=3.1.32\n",
        "- tqdm\n",
        "- setuptools=65.5.1\n",
        "- wheel=0.38.1\n",
        "- openssl=1.1.1s\n",
        "- gunicorn\n",
        "- flask\n",
        "- pip:\n",
        "  - inference-schema\n",
        "  - azure-ai-ml\n",
        "  - azureml-mlflow==1.53.0\n",
        "  - azureml-defaults==1.53.0\n",
        "  - azureml-telemetry==1.53.0\n",
        "  - azureml-interpret==1.53.0\n",
        "  - cryptography>=41.0.2\n",
        "  - scipy==1.10.1\n",
        "  - spacy==2.1.8\n",
        "  - prophet==1.1.4\n",
        "  - py-cpuinfo==5.0.0 "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/dependencies/conda.yaml\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_env_name = \"forecast_pipline_environment\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for forecasting demo pipeline\",\n",
        "    tags={\"scikit-learn\": \"0.24.2\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"1\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name forecast_pipline_environment is registered to workspace, the environment version is 1\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699439237840
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Component 1: Training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_src_dir = \"./src/components/train\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699439238142
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.py\n",
        "\n",
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "from prophet import Prophet\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Define the output directory\n",
        "output_dir = './outputs'\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "# input and output arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--sensor_data\", type=str, help=\"path to sensor data\")\n",
        "parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "sensor_data = pd.read_csv(args.sensor_data)\n",
        "\n",
        "#sensors = sensor_data[\"Sensor Name\"].unique()\n",
        "sensors = ['PER_AIRMON_MESH1907150',\n",
        "        'PER_AIRMON_MESH1971150',\n",
        "        'PER_AIRMON_MESH1904150',\n",
        "        'PER_AIRMON_MESH1966150',\n",
        "        'PER_AIRMON_MESH1911150',\n",
        "        'PER_AIRMON_MESH1964150',\n",
        "        'PER_AIRMON_MESH1974150',\n",
        "        'PER_AIRMON_MESH1909150',\n",
        "        'PER_AIRMON_MONITOR1157100',\n",
        "        'PER_AIRMON_MESH303245',\n",
        "        'PER_AIRMON_MONITOR1156100',\n",
        "        'PER_AIRMON_MONITOR1056100',\n",
        "        'PER_AIRMON_MONITOR1048100',\n",
        "        'PER_AIRMON_MONITOR1135100',\n",
        "        'PER_AIRMON_MESH305245'\n",
        "        ]\n",
        "\n",
        "for sensor in sensors:\n",
        "    try:\n",
        "\n",
        "        '''\n",
        "        clean dataset by replacing suspect readings with the mean average of the dataset\n",
        "\n",
        "        '''\n",
        "        # get the DataFrame for the current sensor\n",
        "        df_sensor = sensor_data[sensor_data[\"Sensor Name\"] == sensor]\n",
        "\n",
        "        # calculate the mean of non-zero, non-negative, and non-empty values\n",
        "        mean_value = df_sensor[df_sensor[\"Value\"] > 0][\"Value\"].mean()\n",
        "\n",
        "        # replace the values for rows that are zero, negative, or na\n",
        "        df_sensor.loc[(df_sensor[\"Value\"] <= 0) | (df_sensor[\"Value\"].isna()), \"Value\"] = mean_value\n",
        "\n",
        "        # replace the empty rows with the mean value\n",
        "        df_sensor[\"Value\"].fillna(mean_value, inplace=True)\n",
        "\n",
        "        # update the original DataFrame with the modified values\n",
        "        sensor_data[sensor_data[\"Sensor Name\"] == sensor] = df_sensor\n",
        "\n",
        "\n",
        "        '''\n",
        "        fit data to model\n",
        "\n",
        "        '''\n",
        "\n",
        "        # get the DataFrame for the current sensor\n",
        "        #column_to_transform = 'Value'\n",
        "        #df_sensor['LogTransformed'] = np.log1p(df_sensor[column_to_transform])\n",
        "        # Create the data for the columns\n",
        "        x = pd.to_datetime(df_sensor[\"Timestamp\"])\n",
        "        #y = df_sensor.LogTransformed\n",
        "        y = df_sensor.Value\n",
        "\n",
        "        # Create a dictionary with the data for each column\n",
        "        data = {'ds': x, 'y': y}\n",
        "\n",
        "        # Create the DataFrame\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Create and fit the Prophet model with custom seasonality\n",
        "        model = Prophet(\n",
        "            daily_seasonality=True,\n",
        "            interval_width=0.8,\n",
        "            changepoint_prior_scale=0.15,\n",
        "            seasonality_prior_scale=0.1,\n",
        "            uncertainty_samples=1000\n",
        "        )\n",
        "\n",
        "        # modify seasonality and fourier_order (daily)\n",
        "        model.add_seasonality(\n",
        "            name='daily',\n",
        "            period=1,\n",
        "            fourier_order=50 \n",
        "        )\n",
        "\n",
        "        # modify seasonality and fourier_order (weekly)\n",
        "        model.add_seasonality(\n",
        "            name='weekly',\n",
        "            period=7,\n",
        "            fourier_order=1\n",
        "        )\n",
        "\n",
        "        # modify seasonality and fourier_order\n",
        "        model.add_seasonality(\n",
        "            name='fortnight',    period=15,\n",
        "            fourier_order=10 \n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        forecast_model = model.fit(df)\n",
        "\n",
        "        '''\n",
        "        save model as a pickle file\n",
        "\n",
        "        '''\n",
        "        # Specify the file path where you want to save the pickle file\n",
        "        file_path = sensor + \"_forecast_model.pkl\"\n",
        "        pickle_file_path = os.path.join(output_dir, file_path)\n",
        "\n",
        "        # Save the model as a pickle file\n",
        "        with open(pickle_file_path, 'wb') as file:\n",
        "            pickle.dump(forecast_model, file)\n",
        "\n",
        "        # Close the file\n",
        "        file.close()\n",
        "\n",
        "        # Specify the file path where you want to save the pickle file\n",
        "        file_path = sensor + \"_forecast_model.pkl\"\n",
        "        pickle_file_path = os.path.join(args.model, file_path)\n",
        "\n",
        "        # Save the model as a pickle file\n",
        "        with open(pickle_file_path, 'wb') as file:\n",
        "            pickle.dump(forecast_model, file)\n",
        "\n",
        "        # Close the file\n",
        "        file.close()\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(sensor + \" has error:\", str(e))\n",
        "        pass           "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/components/train/train.py\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.yml\n",
        "# <component>\n",
        "name: forcast_model\n",
        "display_name: Forecast Model\n",
        "type: command\n",
        "inputs:\n",
        "  sensor_data: \n",
        "    type: uri_folder  \n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: uri_folder\n",
        "code: ./\n",
        "environment:\n",
        "      azureml:forecast_pipeline_env:1\n",
        "command: >-\n",
        "  python train.py \n",
        "  --sensor_data ${{inputs.sensor_data}}  \n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/components/train/train.yml\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the component from the yml file\n",
        "train_component = load_component(source=os.path.join(train_src_dir, \"train.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component forcast_model with Version 2023-11-08-10-27-17-5640180 is registered\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699439239914
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Component 2: Score"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_src_dir = \"./src/components/score\"\n",
        "os.makedirs(score_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699439240103
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {score_src_dir}/score.py\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pathlib import Path\n",
        "import mlflow\n",
        "\n",
        "# start logging\n",
        "mlflow.start_run()\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model\", type=str, help=\"Path to the input model directory\")\n",
        "parser.add_argument(\"--data\", type=str, help=\"Path to the data to score\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "lines = [\n",
        "    f\"model path: {args.model}\",\n",
        "    f\"input data path: {args.data}\",\n",
        "]\n",
        "\n",
        "for line in lines:\n",
        "    print(f\"\\t{line}\")\n",
        "\n",
        "#sensor_data = pd.read_csv(args.data)\n",
        "\n",
        "sensors = ['PER_AIRMON_MESH1907150',\n",
        "        'PER_AIRMON_MESH1971150',\n",
        "        'PER_AIRMON_MESH1904150',\n",
        "        'PER_AIRMON_MESH1966150',\n",
        "        'PER_AIRMON_MESH1911150',\n",
        "        'PER_AIRMON_MESH1964150',\n",
        "        'PER_AIRMON_MESH1974150',\n",
        "        'PER_AIRMON_MESH1909150',\n",
        "        'PER_AIRMON_MONITOR1157100',\n",
        "        'PER_AIRMON_MESH303245',\n",
        "        'PER_AIRMON_MONITOR1156100',\n",
        "        'PER_AIRMON_MONITOR1056100',\n",
        "        'PER_AIRMON_MONITOR1048100',\n",
        "        'PER_AIRMON_MONITOR1135100',\n",
        "        'PER_AIRMON_MESH305245'\n",
        "        ]\n",
        "\n",
        "# Define the output directory for the first directory\n",
        "output_dir = './outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for sensor in sensors:\n",
        "    try:\n",
        "        # Construct the path to the pickle file for the current sensor\n",
        "        model_file_name = f\"{sensor}_forecast_model.pkl\"\n",
        "        model_file_path = os.path.join(args.model, model_file_name)\n",
        "\n",
        "        # Load the trained model for the current sensor\n",
        "        with open(model_file_path, 'rb') as model_file:\n",
        "            model = pickle.load(model_file)\n",
        "\n",
        "        # Generate predictions\n",
        "        future = model.make_future_dataframe(periods=14, freq='D')\n",
        "        forecast = model.predict(future)\n",
        "        fortnight_forecast = forecast.tail(14)\n",
        "        #fortnight_forecast['pred'] = np.expm1(fortnight_forecast['yhat'])\n",
        "        #fortnight_forecast['display_value'] = np.where((fortnight_forecast['pred'] > fortnight_forecast['yhat']) & \n",
        "        #                                               (fortnight_forecast['pred'] < 300), fortnight_forecast['pred'], fortnight_forecast['yhat'])\n",
        "        fortnight_forecast['sensor reading'] = fortnight_forecast['yhat']\n",
        "        fortnight_forecast[\"Sensor Name\"] = sensor\n",
        "    \n",
        "\n",
        "        # Save the predictions to the specified path\n",
        "        output_file_path1 = os.path.join(output_dir, f'{sensor}_predictions.csv')\n",
        "        fortnight_forecast.to_csv(output_file_path1, index=False)\n",
        "\n",
        "        # Plot the forecasted values along with uncertainty range\n",
        "        fig1 = model.plot(forecast)\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Sensor Reading')\n",
        "\n",
        "        # Save the current figure\n",
        "        plot_file_path2 = os.path.join(output_dir, f'{sensor}_prophet_plot.png')\n",
        "        fig1.savefig(plot_file_path2)\n",
        "\n",
        "        # Close the figure to release resources\n",
        "        plt.close(fig1)\n",
        "\n",
        "        # Plot the forecast and confidence interval with light blue color\n",
        "        fig2 = plt.figure(figsize=(12, 6))\n",
        "        plt.plot(fortnight_forecast['ds'], fortnight_forecast['yhat'], label='Forecasted')\n",
        "        plt.fill_between(fortnight_forecast['ds'], fortnight_forecast['yhat_lower'], fortnight_forecast['yhat_upper'], color='lightblue', alpha=0.5, label='Confidence Interval')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Value')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Save the current figure\n",
        "        plot_file_path3 = os.path.join(output_dir, f'{sensor}_prophet_forecast_plot.png')\n",
        "        fig2.savefig(plot_file_path3)\n",
        "\n",
        "        # Close the figure to release resources\n",
        "        plt.close(fig2)\n",
        "\n",
        "        # Plot the components of the forecast\n",
        "        fig3 = model.plot_components(forecast)\n",
        "\n",
        "        # Save the current figure\n",
        "        plot_file_path4 = os.path.join(output_dir, f'{sensor}_prophet_forecast_plot_components.png')\n",
        "        fig3.savefig(plot_file_path4)\n",
        "\n",
        "        # Close the figure to release resources\n",
        "        plt.close(fig3)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{sensor} has error:\", str(e))\n",
        "        pass\n",
        "\n",
        "mlflow.end_run()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/components/score/score.py\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699390862328
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {score_src_dir}/score.yml\n",
        "name: forecast_model_score\n",
        "display_name: Forecast Model Scoring\n",
        "type: command\n",
        "inputs:\n",
        "  model:\n",
        "    type: uri_file\n",
        "  data:\n",
        "    type: uri_file\n",
        "outputs:\n",
        "  scores:\n",
        "    type: uri_file\n",
        "\n",
        "code: ./\n",
        "environment: azureml:forecast_pipeline_env:1\n",
        "command: >-\n",
        "  python score.py\n",
        "  --model ${{inputs.model}}\n",
        "  --data ${{inputs.data}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/components/score/score.yml\n"
        }
      ],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698928126722
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "score_component = load_component(source=os.path.join(score_src_dir, \"score.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "score_component = ml_client.create_or_update(score_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {score_component.name} with Version {score_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading score (0.0 MBs):   0%|          | 0/4380 [00:00<?, ?it/s]\r\u001b[32mUploading score (0.0 MBs): 100%|██████████| 4380/4380 [00:00<00:00, 55487.58it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component forecast_model_score with Version 2023-11-08-12-26-35-4489440 is registered\n"
        }
      ],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699446396497
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Pipeline from Components"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@pipeline\n",
        "def forecasting_pipeline(\n",
        "    pipeline_job_data_input: Input,\n",
        "    pipeline_job_registered_model_name: str,\n",
        "):\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_component(\n",
        "        sensor_data=pipeline_job_data_input,\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # Adding scoring component\n",
        "    score_job = score_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        model=train_job.outputs.model,\n",
        "    )\n",
        "\n",
        "    # A pipeline returns a dictionary of outputs\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": train_job.outputs.model,\n",
        "        \"pipeline_job_score_predictions\": score_job.outputs.scores,\n",
        "    }\n"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699446396798
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"forecasting_model\"\n",
        "\n",
        "# Let's instantiate the pipeline\n",
        "pipeline_job = forecasting_pipeline(\n",
        "    pipeline_job_data_input=Input(type=data_type, path=input_path, mode=input_mode),\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")\n",
        "\n",
        "# set pipeline level compute\n",
        "pipeline_job.settings.default_compute = \"forecasting-compute-cluster\""
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699446397110
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipeline_job)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "display_name: forecasting_pipeline\ntype: pipeline\ninputs:\n  pipeline_job_data_input:\n    mode: ro_mount\n    type: uri_file\n    path: azureml:MasterSourceData:1\n  pipeline_job_registered_model_name: forecasting_model\noutputs:\n  pipeline_job_train_data:\n    type: uri_folder\n  pipeline_job_score_predictions:\n    type: uri_file\njobs:\n  train_job:\n    type: command\n    inputs:\n      sensor_data:\n        path: ${{parent.inputs.pipeline_job_data_input}}\n      registered_model_name:\n        path: ${{parent.inputs.pipeline_job_registered_model_name}}\n    outputs:\n      model: ${{parent.outputs.pipeline_job_train_data}}\n    resources:\n      instance_count: 1\n    component:\n      name: forcast_model\n      version: 2023-11-08-10-27-17-5640180\n      display_name: Forecast Model\n      type: command\n      inputs:\n        sensor_data:\n          type: uri_folder\n          optional: false\n        registered_model_name:\n          type: string\n          optional: false\n      outputs:\n        model:\n          type: uri_folder\n      command: python train.py  --sensor_data ${{inputs.sensor_data}}   --registered_model_name\n        ${{inputs.registered_model_name}}  --model ${{outputs.model}}\n      environment: azureml:/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/environments/forecast_pipeline_env/versions/1\n      code: azureml:/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/codes/f7f8b832-3918-4f23-b87d-300cf590c690/versions/1\n      resources:\n        instance_count: 1\n      id: /subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/components/forcast_model/versions/2023-11-08-10-27-17-5640180\n      creation_context:\n        created_at: '2023-11-08T10:27:18.115143+00:00'\n        created_by: Daniel Godden\n        created_by_type: User\n        last_modified_at: '2023-11-08T10:27:18.195960+00:00'\n        last_modified_by: Daniel Godden\n        last_modified_by_type: User\n      is_deterministic: true\n  score_job:\n    type: command\n    inputs:\n      model:\n        path: ${{parent.jobs.train_job.outputs.model}}\n      data:\n        path: ${{parent.inputs.pipeline_job_data_input}}\n    outputs:\n      scores: ${{parent.outputs.pipeline_job_score_predictions}}\n    resources:\n      instance_count: 1\n    component:\n      name: forecast_model_score\n      version: 2023-11-08-12-26-35-4489440\n      display_name: Forecast Model Scoring\n      type: command\n      inputs:\n        model:\n          type: uri_file\n          optional: false\n        data:\n          type: uri_file\n          optional: false\n      outputs:\n        scores:\n          type: uri_file\n      command: python score.py --model ${{inputs.model}} --data ${{inputs.data}}\n      environment: azureml:/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/environments/forecast_pipeline_env/versions/1\n      code: azureml:/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/codes/122ad701-45a8-4f9f-a9dc-8faf75b243c0/versions/1\n      resources:\n        instance_count: 1\n      id: /subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourceGroups/Peak-rg-MIWG/providers/Microsoft.MachineLearningServices/workspaces/forecasting_demo/components/forecast_model_score/versions/2023-11-08-12-26-35-4489440\n      creation_context:\n        created_at: '2023-11-08T12:26:36.083674+00:00'\n        created_by: Daniel Godden\n        created_by_type: User\n        last_modified_at: '2023-11-08T12:26:36.190172+00:00'\n        last_modified_by: Daniel Godden\n        last_modified_by_type: User\n      is_deterministic: true\nsettings:\n  default_compute: azureml:forecasting-compute-cluster\n\n"
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699446397286
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit Job"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job,\n",
        "    # Project's name\n",
        "    experiment_name=\"forecast_experiment\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: sharp_nail_0byd42w14p\nWeb View: https://ml.azure.com/runs/sharp_nail_0byd42w14p?wsid=/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourcegroups/Peak-rg-MIWG/workspaces/forecasting_demo\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-11-08 12:26:41Z] Completing processing run id 5f4595e6-b890-4624-9fba-23a1a9661403.\n[2023-11-08 12:26:42Z] Submitting 1 runs, first five are: 2a138301:b12ce0f7-7b21-4a5d-8df0-b4fe5b01bfec\n[2023-11-08 12:42:22Z] Completing processing run id b12ce0f7-7b21-4a5d-8df0-b4fe5b01bfec.\n\nExecution Summary\n=================\nRunId: sharp_nail_0byd42w14p\nWeb View: https://ml.azure.com/runs/sharp_nail_0byd42w14p?wsid=/subscriptions/25f559b2-3a60-46ab-bf85-6e8a8359d5e4/resourcegroups/Peak-rg-MIWG/workspaces/forecasting_demo\n\n"
        }
      ],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699447393583
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy to Endpoint"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}